%YAML 1.2
---
title: Add Packed Single-Precision Floating-Point Values
validity: 32,64
opcode:
  - opcode: NP 0F 58 /r
    mnemonic: ADDPS \i{xmm1}, \i{xmm2/m128}
    encoding: LEGACY
    validity:
      32: valid
      64: valid
    cpuid: SSE
    description: >-
      Adds packed single-precision floating-point values from \abbr{xmm2/m128} and \abbr{xmm1}.
      Stores the result in \abbr{xmm1}.
  - opcode: VEX.128.0F.WIG 58 /r
    mnemonic: VADDPS \i{xmm1}, \i{xmm2}, \i{xmm3/m128}
    encoding: VEX
    validity:
      32: valid
      64: valid
    cpuid: AVX
    description: >-
      Adds packed single-precision floating-point values from \abbr{xmm3/m128} and \abbr{xmm2}.
      Stores the result in \abbr{xmm1}.
  - opcode: VEX.256.0F.WIG 58 /r
    mnemonic: VADDPS \i{ymm1}, \i{ymm2}, \i{ymm3/m256}
    encoding: VEX
    validity:
      32: valid
      64: valid
    cpuid: AVX
    description: >-
      Adds packed single-precision floating-point values from \abbr{ymm3/m256} and \abbr{ymm2}.
      Stores the result in \abbr{ymm1}.
  - opcode: EVEX.128.0F.W0 58 /r
    mnemonic: VADDPS \i{xmm1} {k1}{z}, \i{xmm2}, \i{xmm3/m128/m32bcst}
    encoding: EVEX
    validity:
      32: valid
      64: valid
    cpuid:
      - AVX512VL
      - AVX512F
    description: >-
      Adds packed single-precision floating-point values from \abbr{xmm3/m128/m32bcst} and \abbr{xmm2}.
      Stores the result in \abbr{xmm1} using writemask \abbr{k1}.
  - opcode: EVEX.256.0F.W0 58 /r
    mnemonic: VADDPS \i{ymm1} {k1}{z}, \i{ymm2}, \i{ymm3/m256/m32bcst}
    encoding: EVEX
    validity:
      32: valid
      64: valid
    cpuid:
      - AVX512VL
      - AVX512F
    description: >-
      Adds packed single-precision floating-point values from \abbr{ymm3/m256/m32bcst} and \abbr{ymm2}.
      Stores the result in \abbr{ymm1} using writemask \abbr{k1}.
  - opcode: EVEX.512.0F.W0 58 /r
    mnemonic: VADDPS \i{zmm1} {k1}{z}, \i{zmm2}, \i{zmm3/m512/m32bcst} {er}
    encoding: EVEX
    validity:
      32: valid
      64: valid
    cpuid: AVX512F
    description: >-
      Adds packed single-precision floating-point values from \abbr{zmm3/m512/m32bcst} and \abbr{zmm2}.
      Stores the result in \abbr{zmm1} using writemask \abbr{k1}.
encoding:
  operands: 3
  hasTuple: true
  encodings:
    LEGACY:
      - N/A
      - ModRM:reg[rw]
      - ModRM:r/m[r]
      - N/A
    VEX:
      - N/A
      - ModRM:reg[w]
      - VEX.vvvv[r]
      - ModRM:r/m[r]
    EVEX:
      - Full
      - ModRM:reg[w]
      - EVEX.vvvv[r]
      - ModRM:r/m[r]
description: >-
  The \c{(V)ADDPS} instruction adds four, eight, or sixteen single-precision floating-point values from the first source operand to the second source operand and stores the result in the destination operand.

  All versions \i{except} the legacy SSE version zero the unused upper SIMD register bits.
operation: |-
  pub fn addps(dest: &mut Simd<f32>, src: Simd<f32>) {
    dest[31..=0] = dest[31..=0] + src[31..=0];
    dest[63..=32] = dest[63..=32] + src[63..=32];
    dest[95..=64] = dest[95..=64] + src[95..=64];
    dest[127..=96] = dest[127..=96] + src[127..=96];
    // dest[Simd::max()..=128] (unmodified)
  }

  fn vaddpd_vex(dest: &mut Simd<f32>, src1: Simd<f32>, src2: Simd<f32>, kl: u32) {
    for n in 0..kl {
      let vi = n * 32;
      let vi_next = vi + 31;

      dest[vi_next..=vi] = src1[vi_next..=vi] + src2[vi_next..=vi];
    }

    let end = kl * 32;
    dest[Simd::max()..=end] = 0;
  }

  pub fn vaddps_vex128(dest: &mut Simd<f32>, src1: Simd<f32>, src2: Simd<f32>) {
    vaddps_vex(dest, src1, src2, 4);
  }
  pub fn vaddps_vex256(dest: &mut Simd<f32>, src1: Simd<f32>, src2: Simd<f32>) {
    vaddps_vex(dest, src1, src2, 8);
  }

  fn vaddps_evex_mem(dest: &mut Simd<f32>, src1: Simd<f32>, src2: Simd<f32>, k: KMask, kl: u8) {
    for n in 0..kl {
      let vi = n * 32;
      let vi_next = vi + 31;

      if k[n] {
        // broadcast?
        if EVEX.b {
          dest[vi_next..=vi] = src1[vi_next..=vi] + src2[31..=0];
        } else {
          dest[vi_next..=vi] = src1[vi_next..=vi] + src2[vi_next..=vi];
        }
      } else {
        // zero mask?
        if EVEX.z {
          dest[vi_next..=vi] = 0;
        }
      }
    }

    let end = kl * 32;
    dest[Simd::max()..=end] = 0;
  }

  pub fn vaddps_evex128_mem(dest: &mut Simd<f32>, src1: Simd<f32>, src2: Simd<f32>, k: KMask) {
    vaddps_evex_mem(dest, src1, src2, k, 4);
  }
  pub fn vaddps_evex256_mem(dest: &mut Simd<f32>, src1: Simd<f32>, src2: Simd<f32>, k: KMask) {
    vaddps_evex_mem(dest, src1, src2, k, 8);
  }
  pub fn vaddps_evex512_mem(dest: &mut Simd<f32>, src1: Simd<f32>, src2: Simd<f32>, k: KMask) {
    vaddps_evex_mem(dest, src1, src2, k, 16);
  }

  fn vaddps_evex_vreg(dest: &mut Simd<f32>, src1: Simd<f32>, src2: Simd<f32>, k: KMask: kl: u8) {
    set_rounding_for_this_instruction(if kl == 8 && EVEX.b { EVEX.RC } else { MXCSR.RC });

    for n in 0..kl {
      let vi = n * 32;
      let vi_next = vi + 31;

      if k[n] {
        dest[vi_next..=vi] = src1[vi_next..=vi] + src2[vi_next..=vi];
      } else {
        // zero mask?
        if EVEX.z {
          dest[vi_next..=vi] = 0;
        }
      }
    }

    let end = kl * 32;
    dest[Simd::max()..=end] = 0;
  }

  pub fn vaddps_evex128_vreg(dest: &mut Simd<f32>, src1: Simd<f32>, src2: Simd<f32>, k: KMask) {
    vaddps_evex_vreg(dest, src1, src2, k, 4);
  }
  pub fn vaddps_evex256_vreg(dest: &mut Simd<f32>, src1: Simd<f32>, src2: Simd<f32>, k: KMask) {
    vaddps_evex_vreg(dest, src1, src2, k, 8);
  }
  pub fn vaddps_evex512_vreg(dest: &mut Simd<f32>, src1: Simd<f32>, src2: Simd<f32>, k: KMask) {
    vaddps_evex_vreg(dest, src1, src2, k, 16);
  }
intrinsicsC: |-
  __m128 _mm_add_ps(__m128 a, __m128 b)
  __m128 _mm_mask_add_ps(__m128d s, __mmask8 k, __m128 a, __m128 b)
  __m128 _mm_maskz_add_ps(__mmask8 k, __m128 a, __m128 b)

  __m256 _mm256_add_ps(__m256 a, __m256 b)
  __m256 _mm256_mask_add_ps(__m256 s, __mmask8 k, __m256 a, __m256 b)
  __m256 _mm256_maskz_add_ps(__mmask8 k, __m256 a, __m256 b)

  __m512 _mm512_add_ps(__m512 a, __m512 b)
  __m512 _mm512_add_round_ps(__m512 a, __m512 b, int32_t rounding)
  __m512 _mm512_mask_add_ps(__m512 s, __mmask16 k, __m512 a, __m512 b)
  __m512 _mm512_mask_add_round_ps(__m512 s, __mmask16 k, __m512 a, __m512 b, int32_t rounding)
  __m512 _mm512_maskz_add_ps(__mmask16 k, __m512 a, __m512 b)
  __m512 _mm512_maskz_add_round_ps(__mmask16 k, __m512 a, __m512 b, int32_t rounding)
exceptions:
  floating: Overflow, Underflow, Invalid, Precision, Denormal
  other:
    - "VEX encoded form: see Exceptions Type 2."
    - "EVEX encoded form: see Exceptions Type E2."
