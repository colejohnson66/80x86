%YAML 1.2
---
title: Logical AND NOT Packed Double-Precision Floating-Point Values
opcode:
  - opcode: 66 0F 55 /r
    mnemonic: ANDNPD \i{xmm1}, \i{xmm2/m128}
    encoding: LEGACY
    validity:
      16: invalid
      32: valid
      64: valid
    cpuid: SSE2
    description: >-
      ANDs packed double-precision floating-point values from the inverted form of \i{xmm2/m128} and \i{xmm1}.
      Stores the result in \i{xmm1}.
  - opcode: VEX.128.66.0F.WIG 55 /r
    mnemonic: VANDNPD \i{xmm1}, \i{xmm2}, \i{xmm3/m128}
    encoding: VEX
    validity:
      16: invalid
      32: valid
      64: valid
    cpuid: AVX
    description: >-
      ANDs packed double-precision floating-point values from the inverted form of \i{xmm3/m128} and \i{xmm2}.
      Stores the result in \i{xmm1}.
  - opcode: VEX.256.66.0F.WIG 55 /r
    mnemonic: VANDNPD \i{ymm1}, \i{ymm2}, \i{ymm3/m256}
    encoding: VEX
    validity:
      16: invalid
      32: valid
      64: valid
    cpuid: AVX
    description: >-
      ANDs packed double-precision floating-point values from the inverted form of \i{ymm3/m256} and \i{ymm2}.
      Stores the result in \i{ymm1}.
  - opcode: EVEX.128.66.0F.W1 55 /r
    mnemonic: VANDNPD \i{xmm1} {k1}{z}, \i{xmm2}, \i{xmm3/m128/m64bcst}
    encoding: EVEX
    validity:
      16: invalid
      32: valid
      64: valid
    cpuid: [ AVX512VL, AVS512DQ ]
    description: >-
      ANDs packed double-precision floating-point values from the inverted form of \i{xmm3/m128/m64bcst} and \i{xmm2}.
      Stores the result in \i{xmm1}.
  - opcode: EVEX.256.66.0F.W1 55 /r
    mnemonic: VANDNPD \i{ymm1} {k1}{z}, \i{ymm2}, \i{ymm3/m256/m64bcst}
    encoding: EVEX
    validity:
      16: invalid
      32: valid
      64: valid
    cpuid: [ AVX512VL, AVS512DQ ]
    description: >-
      ANDs packed double-precision floating-point values from the inverted form of \i{ymm3/m256/m64bcst} and \i{ymm2}.
      Stores the result in \i{ymm1}.
  - opcode: EVEX.512.66.0F.W1 55 /r
    mnemonic: VANDNPD \i{zmm1} {k1}{z}, \i{zmm2}, \i{zmm3/m512/m64bcst}
    encoding: EVEX
    validity:
      16: invalid
      32: valid
      64: valid
    cpuid: AVX512DQ
    description: >-
      ANDs packed double-precision floating-point values from the inverted form of \i{zmm3/m512/m64bcst} and \i{zmm2}.
      Stores the result in \i{zmm1}.
encoding:
  operands: 3
  hasTuple: true
  encodings:
    LEGACY:
      - N/A
      - ModRM.reg[rw]
      - ModRM.r/m[r]
      - ""
    VEX:
      - N/A
      - ModRM.reg[rw]
      - VEX.vvvv[r]
      - ModRM.r/m[r]
    EVEX:
      - Full
      - ModRM.reg[rw]
      - EVEX.vvvv[r]
      - ModRM.r/m[r]
bitEncoding:
  list:
    - form: xmmreg3(2) into xmmreg3(1)
      bits:
        - \bits{66}
        - \bits{0F}
        - \bits{55}
        - \bits{11 xmmreg3(1) xmmreg3(2)}
    - form: memory into xmmreg3
      bits:
        - \bits{66}
        - \bits{0F}
        - \bits{55}
        - \bits{mod xmmreg3 r/m}
description: >-
  The \c{(V)ANDNPD} instruction ANDs two, four, or eight double-precision floating-point values from the inverted form of the first source operand to the second source operand.
  The result is stored in in the destination operand.

  This instruction, despite being named as if it operates on floating-point numbers, internally operates on 64 bit integers.
  The "Operation" section below has been updated to reflect this (using \c{u64} instead of \c{f64}).

  All versions \i{except} the legacy SSE version zero the unused upper SIMD register bits.
operation: |-
  pub fn andnpd(dest: &mut Simd<u64>, src: Simd<u64>) {
    dest[63..=0] = !dest[63..=0] & src[63..=0];
    dest[127..=64] = !dest[127..=64] & src[127..=64];
    // dest[Simd::max()..=128] (unmodified)
  }

  fn vandnpd_vex(dest: &mut Simd<u64>, src1: Simd<u64>, src2: Simd<u64>, kl: u32) {
    for n in 0..kl {
      let vi = n * 64;
      let vi_next = vi + 63;

      dest[vi_next..=vi] = !src1[vi_next..=vi] & src2[vi_next..=vi];
    }

    let end = kl * 64;
    dest[Simd::max()..=end] = 0;
  }

  pub fn vandnpd_vex128(dest: &mut Simd<u64>, src1: Simd<u64>, src2: Simd<u64>) {
    vandnpd_vex(dest, src1, src2, 2);
  }
  pub fn vandnpd_vex256(dest: &mut Simd<u64>, src1: Simd<u64>, src2: Simd<u64>) {
    vandnpd_vex(dest, src1, src2, 4);
  }

  fn vandnpd_evex_mem(dest: &mut Simd<u64>, src1: Simd<u64>, src2: Simd<u64>, k: KMask, kl: u8) {
    for n in 0..kl {
      let vi = n * 64;
      let vi_next = vi + 63;

      if k[n] {
        // broadcast?
        if EVEX.b {
          dest[vi_next..=vi] = !src1[vi_next..=vi] & src2[63..=0];
        } else {
          dest[vi_next..=vi] = !src1[vi_next..=vi] & src2[vi_next..=vi];
        }
      } else {
        // zero mask?
        if EVEX.z {
          dest[vi_next..=vi] = 0;
        }
      }
    }

    let end = kl * 64;
    dest[Simd::max()..=end] = 0;
  }

  pub fn vandnpd_evex128_mem(dest: &mut Simd<u64>, src1: Simd<u64>, src2: Simd<u64>, k: KMask) {
    vandnpd_evex_mem(dest, src1, src2, k, 2);
  }
  pub fn vandnpd_evex256_mem(dest: &mut Simd<u64>, src1: Simd<u64>, src2: Simd<u64>, k: KMask) {
    vandnpd_evex_mem(dest, src1, src2, k, 4);
  }
  pub fn vandnpd_evex512_mem(dest: &mut Simd<u64>, src1: Simd<u64>, src2: Simd<u64>, k: KMask) {
    vandnpd_evex_mem(dest, src1, src2, k, 8);
  }

  fn vandnpd_evex_vreg(dest: &mut Simd<u64>, src1: Simd<u64>, src2: Simd<u64>, k: KMask: kl: u8) {
    for n in 0..kl {
      let vi = n * 64;
      let vi_next = vi + 63;

      if k[n] {
        dest[vi_next..=vi] = !src1[vi_next..=vi] & src2[vi_next..=vi];
      } else {
        // zero mask?
        if EVEX.z {
          dest[vi_next..=vi] = 0;
        }
      }
    }

    let end = kl * 64;
    dest[Simd::max()..=end] = 0;
  }

  pub fn vandnpd_evex128_vreg(dest: &mut Simd<u64>, src1: Simd<u64>, src2: Simd<u64>, k: KMask) {
    vandnpd_evex_vreg(dest, src1, src2, k, 2);
  }
  pub fn vandnpd_evex256_vreg(dest: &mut Simd<u64>, src1: Simd<u64>, src2: Simd<u64>, k: KMask) {
    vandnpd_evex_vreg(dest, src1, src2, k, 4);
  }
  pub fn vandnpd_evex512_vreg(dest: &mut Simd<u64>, src1: Simd<u64>, src2: Simd<u64>, k: KMask) {
    vandnpd_evex_vreg(dest, src1, src2, k, 8);
  }
intrinsicsC: |-
  __m128d _mm_andnot_pd(__m128d a, __m128d b)
  __m128d _mm_mask_andnot_pd(__m128d s, __mmask8 k, __m128d a, __m128d b)
  __m128d _mm_maskz_andnot_pd(__mmask8 k, __m128d a, __m128d b)

  __m256d _mm256_andnot_pd(__m256d a, __m256d b)
  __m256d _mm256_mask_andnot_pd(__m256d s, __mmask8 k, __m256d a, __m256d b)
  __m256d _mm256_maskz_andnot_pd(__mmask8 k, __m256d a, __m256d b)

  __m512d _mm512_andnot_pd(__m512d a, __m512d b)
  __m512d _mm512_mask_andnot_pd(__m512d s, __mmask8 k, __m512d a, __m512d b)
  __m512d _mm512_maskz_andnot_pd(__mmask8 k, __m512d a, __m512d b)
exceptions:
  floating: None
  other:
    - "VEX encoded form: see Exceptions Type 4."
    - "EVEX encoded form: see Exceptions Type E4."
changes:
  version: 73
  date: 2020-11
  list:
    - The VEX form mnemonics (\c{VEX.###.66.0F.WIG 55 /r}) are incorrectly written as \c{VEX.###.66.0F 55 /r} (no \c{WIG})
