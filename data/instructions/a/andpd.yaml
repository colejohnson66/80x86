%YAML 1.2
---
title: Logical AND Packed Double-Precision Floating-Point Values
validity: 32,64
opcode:
  - opcode: 66 0F 54 /r
    mnemonic: ANDPD \i{xmm1}, \i{xmm2/m128}
    encoding: LEGACY
    validity:
      32: valid
      64: valid
    cpuid: SSE2
    description: >-
      ANDs packed double-precision floating-point values from \i{xmm2/m128} and \i{xmm1}.
      Stores the result in \i{xmm1}.
  - opcode: VEX.128.66.0F.WIG 54 /r
    mnemonic: VANDPD \i{xmm1}, \i{xmm2}, \i{xmm3/m128}
    encoding: VEX
    validity:
      32: valid
      64: valid
    cpuid: AVX
    description: >-
      ANDs packed double-precision floating-point values from \i{xmm3/m128} and \i{xmm2}.
      Stores the result in \i{xmm1}.
  - opcode: VEX.256.66.0F.WIG 54 /r
    mnemonic: VANDPD \i{ymm1}, \i{ymm2}, \i{ymm3/m256}
    encoding: VEX
    validity:
      32: valid
      64: valid
    cpuid: AVX
    description: >-
      ANDs packed double-precision floating-point values from \i{ymm3/m256} and \i{ymm2}.
      Stores the result in \i{ymm1}.
  - opcode: EVEX.128.66.0F.W1 54 /r
    mnemonic: VANDPD \i{xmm1} {k1}{z}, \i{xmm2}, \i{xmm3/m128/m64bcst}
    encoding: EVEX
    validity:
      32: valid
      64: valid
    cpuid: [ AVX512VL, AVS512DQ ]
    description: >-
      ANDs packed double-precision floating-point values from \i{xmm3/m128/m64bcst} and \i{xmm2}.
      Stores the result in \i{xmm1}.
  - opcode: EVEX.256.66.0F.W1 54 /r
    mnemonic: VANDPD \i{ymm1} {k1}{z}, \i{ymm2}, \i{ymm3/m256/m64bcst}
    encoding: EVEX
    validity:
      32: valid
      64: valid
    cpuid: [ AVX512VL, AVS512DQ ]
    description: >-
      ANDs packed double-precision floating-point values from \i{ymm3/m256/m64bcst} and \i{ymm2}.
      Stores the result in \i{ymm1}.
  - opcode: EVEX.512.66.0F.W1 54 /r
    mnemonic: VANDPD \i{zmm1} {k1}{z}, \i{zmm2}, \i{zmm3/m512/m64bcst}
    encoding: EVEX
    validity:
      32: valid
      64: valid
    cpuid: AVX512DQ
    description: >-
      ANDs packed double-precision floating-point values from \i{zmm3/m512/m64bcst} and \i{zmm2}.
      Stores the result in \i{zmm1}.
encoding:
  operands: 3
  hasTuple: true
  encodings:
    LEGACY:
      - N/A
      - ModRM.reg[rw]
      - ModRM.r/m[r]
      - ""
    VEX:
      - N/A
      - ModRM.reg[rw]
      - VEX.vvvv[r]
      - ModRM.r/m[r]
    EVEX:
      - Full
      - ModRM.reg[rw]
      - EVEX.vvvv[r]
      - ModRM.r/m[r]
bitEncoding:
  list:
    - form: xmmreg3(2) into xmmreg3(1)
      bits:
        - \bits{66}
        - \bits{0F}
        - \bits{54}
        - \bits{11 xmmreg3(1) xmmreg3(2)}
    - form: memory into xmmreg3
      bits:
        - \bits{66}
        - \bits{0F}
        - \bits{54}
        - \bits{mod xmmreg3 r/m}
description: >-
  The \c{(V)ANDPD} instruction ANDs two, four, or eight double-precision floating-point values from the first source operand to the second source operand.
  The result is stored in in the destination operand.

  This instruction, despite being named as if it operates on floating-point numbers, internally operates on 64 bit integers.
  The "Operation" section below has been updated to reflect this (using \c{u64} instead of \c{f64}).

  All versions \i{except} the legacy SSE version zero the unused upper SIMD register bits.
operation: |-
  pub fn andpd(dest: &mut Simd<u64>, src: Simd<u64>) {
    dest[63..=0] = dest[63..=0] & src[63..=0];
    dest[127..=64] = dest[127..=64] & src[127..=64];
    // dest[Simd::max()..=128] (unmodified)
  }

  fn vandpd_vex(dest: &mut Simd<u64>, src1: Simd<u64>, src2: Simd<u64>, kl: u32) {
    for n in 0..kl {
      let vi = n * 64;
      let vi_next = vi + 63;

      dest[vi_next..=vi] = src1[vi_next..=vi] & src2[vi_next..=vi];
    }

    let end = kl * 64;
    dest[Simd::max()..=end] = 0;
  }

  pub fn vandpd_vex128(dest: &mut Simd<u64>, src1: Simd<u64>, src2: Simd<u64>) {
    vandpd_vex(dest, src1, src2, 2);
  }
  pub fn vandpd_vex256(dest: &mut Simd<u64>, src1: Simd<u64>, src2: Simd<u64>) {
    vandpd_vex(dest, src1, src2, 4);
  }

  fn vandpd_evex_mem(dest: &mut Simd<u64>, src1: Simd<u64>, src2: Simd<u64>, k: KMask, kl: u8) {
    for n in 0..kl {
      let vi = n * 64;
      let vi_next = vi + 63;

      if k[n] {
        // broadcast?
        if EVEX.b {
          dest[vi_next..=vi] = src1[vi_next..=vi] & src2[63..=0];
        } else {
          dest[vi_next..=vi] = src1[vi_next..=vi] & src2[vi_next..=vi];
        }
      } else {
        // zero mask?
        if EVEX.z {
          dest[vi_next..=vi] = 0;
        }
      }
    }

    let end = kl * 64;
    dest[Simd::max()..=end] = 0;
  }

  pub fn vandpd_evex128_mem(dest: &mut Simd<u64>, src1: Simd<u64>, src2: Simd<u64>, k: KMask) {
    vandpd_evex_mem(dest, src1, src2, k, 2);
  }
  pub fn vandpd_evex256_mem(dest: &mut Simd<u64>, src1: Simd<u64>, src2: Simd<u64>, k: KMask) {
    vandpd_evex_mem(dest, src1, src2, k, 4);
  }
  pub fn vandpd_evex512_mem(dest: &mut Simd<u64>, src1: Simd<u64>, src2: Simd<u64>, k: KMask) {
    vandpd_evex_mem(dest, src1, src2, k, 8);
  }

  fn vandpd_evex_vreg(dest: &mut Simd<u64>, src1: Simd<u64>, src2: Simd<u64>, k: KMask: kl: u8) {
    for n in 0..kl {
      let vi = n * 64;
      let vi_next = vi + 63;

      if k[n] {
        dest[vi_next..=vi] = src1[vi_next..=vi] & src2[vi_next..=vi];
      } else {
        // zero mask?
        if EVEX.z {
          dest[vi_next..=vi] = 0;
        }
      }
    }

    let end = kl * 64;
    dest[Simd::max()..=end] = 0;
  }

  pub fn vandpd_evex128_vreg(dest: &mut Simd<u64>, src1: Simd<u64>, src2: Simd<u64>, k: KMask) {
    vandpd_evex_vreg(dest, src1, src2, k, 2);
  }
  pub fn vandpd_evex256_vreg(dest: &mut Simd<u64>, src1: Simd<u64>, src2: Simd<u64>, k: KMask) {
    vandpd_evex_vreg(dest, src1, src2, k, 4);
  }
  pub fn vandpd_evex512_vreg(dest: &mut Simd<u64>, src1: Simd<u64>, src2: Simd<u64>, k: KMask) {
    vandpd_evex_vreg(dest, src1, src2, k, 8);
  }
intrinsicsC: |-
  __m128d _mm_and_pd(__m128d a, __m128d b)
  __m128d _mm_mask_and_pd(__m128d s, __mmask8 k, __m128d a, __m128d b)
  __m128d _mm_maskz_and_pd(__mmask8 k, __m128d a, __m128d b)

  __m256d _mm256_and_pd(__m256d a, __m256d b)
  __m256d _mm256_mask_and_pd(__m256d s, __mmask8 k, __m256d a, __m256d b)
  __m256d _mm256_maskz_and_pd(__mmask8 k, __m256d a, __m256d b)

  __m512d _mm512_and_pd(__m512d a, __m512d b)
  __m512d _mm512_mask_and_pd(__m512d s, __mmask8 k, __m512d a, __m512d b)
  __m512d _mm512_maskz_and_pd(__mmask8 k, __m512d a, __m512d b)
exceptions:
  floating: None
  other:
    - "VEX encoded form: see Exceptions Type 4."
    - "EVEX encoded form: see Exceptions Type E4."
